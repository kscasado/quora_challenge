{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dropout, Dense, Flatten, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import Merge,Input,concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('quora_duplicate_questions.tsv',sep='\\t',encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#from gensim.utils import lemmatize\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(string):\n",
    "    #try: \n",
    "    rx = re.compile('\\W+')\n",
    "    res = rx.sub(' ',string).strip()\n",
    "    res = res.replace('\\'','')\n",
    "    #return ' '.join([str(x) for x in nlp(res)])\n",
    "    #return ' '.join([x.lemma_ for x in nlp(res)])\n",
    "\n",
    "    return ' '.join([x.lemma_ for x in nlp(res) if not x.is_stop ])\n",
    "\n",
    "    #except TypeError:\n",
    "        #print('TypeError',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['q1_clean'] = df['question1'].apply(lambda x: clean_data(x))\n",
    "df['q2_clean'] = df['question2'].apply(lambda x: clean_data(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_clean</th>\n",
       "      <th>q2_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>step step guide invest share market india</td>\n",
       "      <td>step step guide invest share market</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>story kohinoor koh noor diamond</td>\n",
       "      <td>happen indian government steal kohinoor koh no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>increase speed internet connection vpn</td>\n",
       "      <td>internet speed increase hack dns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>mentally lonely solve</td>\n",
       "      <td>find remainder math 23 24 math divide 24 23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>dissolve water quikly sugar salt methane carbo...</td>\n",
       "      <td>fish survive salt water</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "1  What would happen if the Indian government sto...             0   \n",
       "2  How can Internet speed be increased by hacking...             0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0   \n",
       "4            Which fish would survive in salt water?             0   \n",
       "\n",
       "                                            q1_clean  \\\n",
       "0          step step guide invest share market india   \n",
       "1                    story kohinoor koh noor diamond   \n",
       "2             increase speed internet connection vpn   \n",
       "3                              mentally lonely solve   \n",
       "4  dissolve water quikly sugar salt methane carbo...   \n",
       "\n",
       "                                            q2_clean  \n",
       "0                step step guide invest share market  \n",
       "1  happen indian government steal kohinoor koh no...  \n",
       "2                   internet speed increase hack dns  \n",
       "3        find remainder math 23 24 math divide 24 23  \n",
       "4                            fish survive salt water  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q1_counts \n",
      " count    404288.000000\n",
      "mean          5.222468\n",
      "std           2.913420\n",
      "min           0.000000\n",
      "25%           3.000000\n",
      "50%           5.000000\n",
      "75%           6.000000\n",
      "max          68.000000\n",
      "Name: q1_clean, dtype: float64\n",
      "q2_counts \n",
      " count    404288.000000\n",
      "mean          5.278838\n",
      "std           3.182402\n",
      "min           0.000000\n",
      "25%           3.000000\n",
      "50%           4.000000\n",
      "75%           6.000000\n",
      "max          92.000000\n",
      "Name: q2_clean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('q1_counts \\n',df['q1_clean'].apply(lambda x: len(x.split())).describe())\n",
    "print('q2_counts \\n',df['q2_clean'].apply(lambda x: len(x.split())).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(string):\n",
    "    for x in string.split():\n",
    "        vocab.add(x)\n",
    "        lexicon.append(x)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "lexicon = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = df['q1_clean'].apply(lambda x: build_vocab(x))\n",
    "_ = df['q2_clean'].apply(lambda x: build_vocab(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77092"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_labels(inputs, cnter,max_len,vocab_len):\n",
    "    np_input = []\n",
    "    \n",
    "    for sentences in inputs:\n",
    "        sen  = np.full(max_len,(vocab_len))\n",
    "        for idx,word in enumerate(sentences.split()):\n",
    "            if(idx<max_len):\n",
    "                sen[idx] = cnter.index(word)\n",
    "        np_input.append(sen)\n",
    "        \n",
    "    return np.array(np_input).astype(np.int)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "vocab_cnter = Counter(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_vocab = [x[0] for x in vocab_cnter.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good', 's', 'india', 'like', 'people', 'way', 't', 'quora', 'learn', 'life']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = []\n",
    "for word in sorted_vocab:\n",
    "    word2vec.append(nlp(word).vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77092"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77093, 300)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.append(np.zeros(300))\n",
    "word2vec = np.array(word2vec)\n",
    "word2vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q1 processed\n"
     ]
    }
   ],
   "source": [
    "q1_inputs = generate_labels(df['q1_clean'],sorted_vocab,30,len(vocab))\n",
    "print('q1 processed')\n",
    "q2_inputs = generate_labels(df['q2_clean'],sorted_vocab,30,len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A keras attention layer that wraps RNN layers.\n",
    "Based on tensorflows [attention_decoder](https://github.com/tensorflow/tensorflow/blob/c8a45a8e236776bed1d14fd71f3b6755bd63cc58/tensorflow/python/ops/seq2seq.py#L506) \n",
    "and [Grammar as a Foreign Language](https://arxiv.org/abs/1412.7449).\n",
    "date: 20161101\n",
    "author: wassname\n",
    "url: https://gist.github.com/wassname/5292f95000e409e239b9dc973295327a\n",
    "\"\"\"\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec\n",
    "from keras.layers import LSTM, activations, Wrapper, Recurrent\n",
    "\n",
    "class Attention(Wrapper):\n",
    "    \"\"\"\n",
    "    This wrapper will provide an attention layer to a recurrent layer. \n",
    "    \n",
    "    # Arguments:\n",
    "        layer: `Recurrent` instance with consume_less='gpu' or 'mem'\n",
    "    \n",
    "    # Examples:\n",
    "    \n",
    "    ```python\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(10, return_sequences=True), batch_input_shape=(4, 5, 10))\n",
    "    model.add(TFAttentionRNNWrapper(LSTM(10, return_sequences=True, consume_less='gpu')))\n",
    "    model.add(Dense(5))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop') \n",
    "    ```\n",
    "    \n",
    "    # References\n",
    "    - [Grammar as a Foreign Language](https://arxiv.org/abs/1412.7449)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, layer, **kwargs):\n",
    "        assert isinstance(layer, Recurrent)\n",
    "        #if layer.get_config()['consume_less']=='cpu':\n",
    "            #raise Exception(\"AttentionLSTMWrapper doesn't support RNN's with consume_less='cpu'\")\n",
    "        self.supports_masking = True\n",
    "        super(Attention, self).__init__(layer, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 3\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        nb_samples, nb_time, input_dim = input_shape\n",
    "\n",
    "        if not self.layer.built:\n",
    "            self.layer.build(input_shape)\n",
    "            self.layer.built = True\n",
    "\n",
    "        super(Attention, self).build()\n",
    "        \n",
    "        self.W1 = self.layer.init((input_dim, input_dim, 1, 1), name='{}_W1'.format(self.name))\n",
    "        self.W2 = self.layer.init((self.layer.output_dim, input_dim), name='{}_W2'.format(self.name))\n",
    "        self.b2 = K.zeros((input_dim,), name='{}_b2'.format(self.name))\n",
    "        self.W3 = self.layer.init((input_dim*2, input_dim), name='{}_W3'.format(self.name))\n",
    "        self.b3 = K.zeros((input_dim,), name='{}_b3'.format(self.name))\n",
    "        self.V = self.layer.init((input_dim,), name='{}_V'.format(self.name))\n",
    "\n",
    "        self.trainable_weights = [self.W1, self.W2, self.W3, self.V, self.b2, self.b3]\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.layer.get_output_shape_for(input_shape)\n",
    "\n",
    "    def step(self, x, states):\n",
    "        # This is based on [tensorflows implementation](https://github.com/tensorflow/tensorflow/blob/c8a45a8e236776bed1d14fd71f3b6755bd63cc58/tensorflow/python/ops/seq2seq.py#L506).\n",
    "        # First, we calculate new attention masks:\n",
    "        #   attn = softmax(V^T * tanh(W2 * X +b2 + W1 * h))\n",
    "        # and we make the input as a concatenation of the input and weighted inputs which is then\n",
    "        # transformed back to the shape x of using W3\n",
    "        #   x = W3*(x+X*attn)+b3\n",
    "        # Then, we run the cell on a combination of the input and previous attention masks:\n",
    "        #   h, state = cell(x, h).\n",
    "        \n",
    "        nb_samples, nb_time, input_dim = self.input_spec[0].shape\n",
    "        h = states[0]\n",
    "        X = states[-1]\n",
    "        xW1 = states[-2]\n",
    "        \n",
    "        Xr = K.reshape(X,(-1,nb_time,1,input_dim))\n",
    "        hW2 = K.dot(h,self.W2)+self.b2\n",
    "        hW2 = K.reshape(hW2,(-1,1,1,input_dim)) \n",
    "        u = K.tanh(xW1+hW2)\n",
    "        a = K.sum(self.V*u,[2,3])\n",
    "        a = K.softmax(a)\n",
    "        a = K.reshape(a,(-1, nb_time, 1, 1))\n",
    "        \n",
    "        # Weight attention vector by attention\n",
    "        Xa = K.sum(a*Xr,[1,2])\n",
    "        Xa = K.reshape(Xa,(-1,input_dim))\n",
    "        \n",
    "        # Merge input and attention weighted inputs into one vector of the right size.\n",
    "        x = K.dot(K.concatenate([x,Xa],1),self.W3)+self.b3    \n",
    "        \n",
    "        h, new_states = self.layer.step(x, states)\n",
    "        return h, new_states\n",
    "\n",
    "    def get_constants(self, x):\n",
    "        constants = self.layer.get_constants(x)\n",
    "        \n",
    "        # Calculate K.dot(x, W2) only once per sequence by making it a constant\n",
    "        nb_samples, nb_time, input_dim = self.input_spec[0].shape\n",
    "        Xr = K.reshape(x,(-1,nb_time,input_dim,1))\n",
    "        Xrt = K.permute_dimensions(Xr, (0, 2, 1, 3))\n",
    "        xW1t = K.conv2d(Xrt,self.W1,border_mode='same')     \n",
    "        xW1 = K.permute_dimensions(xW1t, (0, 2, 3, 1))\n",
    "        constants.append(xW1)\n",
    "        \n",
    "        # we need to supply the full sequence of inputs to step (as the attention_vector)\n",
    "        constants.append(x)\n",
    "        \n",
    "        return constants\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # input shape: (nb_samples, time (padded with zeros), input_dim)\n",
    "        input_shape = self.input_spec[0].shape\n",
    "        if K._BACKEND == 'tensorflow':\n",
    "            if not input_shape[1]:\n",
    "                raise Exception('When using TensorFlow, you should define '\n",
    "                                'explicitly the number of timesteps of '\n",
    "                                'your sequences.\\n'\n",
    "                                'If your first layer is an Embedding, '\n",
    "                                'make sure to pass it an \"input_length\" '\n",
    "                                'argument. Otherwise, make sure '\n",
    "                                'the first layer has '\n",
    "                                'an \"input_shape\" or \"batch_input_shape\" '\n",
    "                                'argument, including the time axis. '\n",
    "                                'Found input shape at layer ' + self.name +\n",
    "                                ': ' + str(input_shape))\n",
    "\n",
    "        if self.layer.stateful:\n",
    "            initial_states = self.layer.states\n",
    "        else:\n",
    "            initial_states = self.layer.get_initial_states(x)\n",
    "        constants = self.get_constants(x)\n",
    "        preprocessed_input = self.layer.preprocess_input(x)\n",
    "        \n",
    "\n",
    "        last_output, outputs, states = K.rnn(self.step, preprocessed_input,\n",
    "                                             initial_states,\n",
    "                                             go_backwards=self.layer.go_backwards,\n",
    "                                             mask=mask,\n",
    "                                             constants=constants,\n",
    "                                             unroll=self.layer.unroll,\n",
    "                                             input_length=input_shape[1])\n",
    "        if self.layer.stateful:\n",
    "            self.updates = []\n",
    "            for i in range(len(states)):\n",
    "                self.updates.append((self.layer.states[i], states[i]))\n",
    "\n",
    "        if self.layer.return_sequences:\n",
    "            return outputs\n",
    "        else:\n",
    "            return last_output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    :param kwargs:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializations.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.dot(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        mul_a = uit  * self.u # with this\n",
    "        ait = K.sum(mul_a, axis=2) # and this\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"Shape transformation logic so Keras can infer output shape\n",
    "        \"\"\"\n",
    "        return (input_shape[0], input_shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers.core import  Lambda,Dropout,Dense, Flatten, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.recurrent import GRU,LSTM\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import Concatenate, Input, concatenate \n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import initializers as initializations\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "from keras import backend as K\n",
    "def cosine_distance(vests):\n",
    "    x, y = vests\n",
    "    x = K.l2_normalize(x, axis=-1)\n",
    "    y = K.l2_normalize(y, axis=-1)\n",
    "    return -K.mean(x * y, axis=-1, keepdims=True)\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n",
    "\n",
    "def cos_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0],1)\n",
    "\n",
    "# Q1\n",
    "input_1 = Input(shape=(30,))\n",
    "embedd = Embedding(len(sorted_vocab)+1,300,\n",
    "                        weights =[word2vec],\n",
    "                        name=\"Embedding_1\")\n",
    "embedding_1 = (embedd)(input_1)\n",
    "\n",
    "bi_gru_1 = Bidirectional(LSTM(200,name='LSTM_1',dropout=.8))(embedding_1)\n",
    "#att_1 = AttentionWithContext()(bi_gru_1)\n",
    "#bi_gru_1_2 =  Bidirectional(LSTM(100,name=\"LSTM_1_2\",dropout=.8,recurrent_dropout=.8,return_sequences=True))(bi_gru_1)\n",
    "#bi_gru_1_3 =  Bidirectional(LSTM(50,name=\"LSTM_1_2\",dropout=.8,recurrent_dropout=.8))(bi_gru_1_2)\n",
    "#Q2 \n",
    "input_2 = Input(shape=(30,))\n",
    "embedding_2 = (embedd)(input_2)\n",
    "bi_gru_2 = Bidirectional(LSTM(200,name=\"LSTM_2\",dropout=.8))(embedding_2)\n",
    "#att_2 = AttentionWithContext()(bi_gru_2)\n",
    "#bi_gru_2_2 =  Bidirectional(LSTM(100,name=\"LSTM_2_2\",recurrent_dropout=.8,return_sequences=True))(bi_gru_2)\n",
    "#bi_gru_2_3 = Bidirectional(LSTM(50,name='LSTM_2_3'))(bi_gru_2_2)\n",
    "\n",
    "#Merge\n",
    "#cosine distance\n",
    "#merged_1 = Lambda(cosine_distance,output_shape=cos_dist_output_shape)([att_1,att_2])\n",
    "#euclid distance\n",
    "#merged_1 = Flatten()(merged_1)\n",
    "#print(merged_1)\n",
    "#merged_2 = Lambda(euclidean_distance,output_shape=eucl_dist_output_shape)([att_1,att_2])\n",
    "#merged_2 = Flatten()(merged_2)\n",
    "merged = concatenate([bi_gru_1,bi_gru_2])\n",
    "#merged = (Flatten())(merged)\n",
    "#Dense\n",
    "dense = Dense(4098,activation='relu',name='dense')(merged)\n",
    "drop_1 = Dropout(.5,name='drop_1')(dense)\n",
    "drop_1 = BatchNormalization()(drop_1)\n",
    "dense_2 = Dense(1000,activation='relu',name='dense_2')(drop_1)\n",
    "drop_2 = Dropout(.5,name='drop_2')(dense_2)\n",
    "drop_2 = BatchNormalization()(drop_2)\n",
    "pred = Dense(2,activation=\"softmax\",name='pred')(drop_2)\n",
    "model = Model(inputs=[input_1,input_2],outputs=pred)\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(len(vocab),64,input_length=sentence_len,name='Embedding_Input'))\n",
    "# model.add(Bidirectional(GRU(100,dropout=.5,name='GRU')))\n",
    "# model.add(Dense(1000,activation='relu',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "Embedding_1 (Embedding)          (None, 30, 300)       23127900                                     \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 400)           801600                                       \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 400)           801600                                       \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 800)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense (Dense)                    (None, 4098)          3282498                                      \n",
      "____________________________________________________________________________________________________\n",
      "drop_1 (Dropout)                 (None, 4098)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 4098)          16392                                        \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1000)          4099000                                      \n",
      "____________________________________________________________________________________________________\n",
      "drop_2 (Dropout)                 (None, 1000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 1000)          4000                                         \n",
      "____________________________________________________________________________________________________\n",
      "pred (Dense)                     (None, 2)             2002                                         \n",
      "====================================================================================================\n",
      "Total params: 32,134,992.0\n",
      "Trainable params: 32,124,796.0\n",
      "Non-trainable params: 10,196.0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404288, 30)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404288, 30)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1_score(tags, predicted):\n",
    "    #print(tags)\n",
    "    #print(K.softmax(predicted))\n",
    "    tags = set(tags)\n",
    "    #print(tags)\n",
    "    predicted = set(K.(predicted))\n",
    "\n",
    "    tp = len(tags & predicted)\n",
    "    fp = len(predicted) - tp \n",
    "    fn = len(tags) - tp\n",
    "\n",
    "    if tp>0:\n",
    "        precision=float(tp)/(tp+fp)\n",
    "        recall=float(tp)/(tp+fn)\n",
    "        return 2*((precision*recall)/(precision+recall))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 363859 samples, validate on 40429 samples\n",
      "Epoch 1/100\n",
      "363859/363859 [==============================] - 211s - loss: 0.7111 - acc: 0.6227 - val_loss: 0.5920 - val_acc: 0.7025\n",
      "Epoch 2/100\n",
      "363859/363859 [==============================] - 208s - loss: 0.5939 - acc: 0.6879 - val_loss: 0.5508 - val_acc: 0.7211\n",
      "Epoch 3/100\n",
      "363859/363859 [==============================] - 210s - loss: 0.5650 - acc: 0.7106 - val_loss: 0.5408 - val_acc: 0.7315\n",
      "Epoch 4/100\n",
      "363859/363859 [==============================] - 214s - loss: 0.5437 - acc: 0.7258 - val_loss: 0.5402 - val_acc: 0.7229\n",
      "Epoch 5/100\n",
      "363859/363859 [==============================] - 215s - loss: 0.5280 - acc: 0.7358 - val_loss: 0.5110 - val_acc: 0.7473\n",
      "Epoch 6/100\n",
      "363859/363859 [==============================] - 217s - loss: 0.5146 - acc: 0.7442 - val_loss: 0.5078 - val_acc: 0.7500\n",
      "Epoch 7/100\n",
      "363859/363859 [==============================] - 217s - loss: 0.5023 - acc: 0.7518 - val_loss: 0.4963 - val_acc: 0.7564\n",
      "Epoch 8/100\n",
      "363859/363859 [==============================] - 218s - loss: 0.4923 - acc: 0.7573 - val_loss: 0.4984 - val_acc: 0.7531\n",
      "Epoch 9/100\n",
      "363859/363859 [==============================] - 218s - loss: 0.4835 - acc: 0.7618 - val_loss: 0.4816 - val_acc: 0.7639\n",
      "Epoch 10/100\n",
      "363859/363859 [==============================] - 218s - loss: 0.4744 - acc: 0.7679 - val_loss: 0.4792 - val_acc: 0.7679\n",
      "Epoch 11/100\n",
      "363859/363859 [==============================] - 219s - loss: 0.4666 - acc: 0.7716 - val_loss: 0.4697 - val_acc: 0.7685\n",
      "Epoch 12/100\n",
      " 86016/363859 [======>.......................] - ETA: 164s - loss: 0.4626 - acc: 0.7741"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "adam = optimizers.Adam(lr=.0001)\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph_2_lstm_with_f1', histogram_freq=0, write_graph=True, write_images=True)\n",
    "x_train_1,x_test_1,x_train_2,x_test_2,y_train,y_test = train_test_split(q1_inputs,q2_inputs,pd.get_dummies(df['is_duplicate']).values,test_size=.1,random_state=4)\n",
    "model.compile(loss='binary_crossentropy',optimizer=adam,metrics=['accuracy'])\n",
    "model.fit(x=[x_train_1,x_train_2],y=y_train,\n",
    "          batch_size=256,epochs=100,validation_data=[[x_test_1,x_test_2],y_test],callbacks=[tbCallBack,ea])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "def train_loop(model, epochs,feats,labels):\n",
    "    #permutation = np.array.permutation(len)\n",
    "    #print(feats)\n",
    "    x_train_1,x_train_2,x_test_1,x_test_2,y_train,y_test = train_test_split(feats[0],feats[1],labels,test_size=.1,random_state=4)\n",
    "    for i in range(epochs):\n",
    "        model.fit(x=x_train,y=y_train, batch_size=64,ephocs=1)\n",
    "        print('F1:',metrics.f1_score(y_test,(model.predict(x_test))))\n",
    "\n",
    "    model.save('LSTM_3.p')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
